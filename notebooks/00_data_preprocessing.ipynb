{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95147947",
   "metadata": {},
   "source": [
    "# KDD Cup 2022 ESCI Challenge - Data Preprocessing\n",
    "\n",
    "Bu notebook, Shopping Queries Dataset'ini preprocessing yapmak iÃ§in kullanÄ±lÄ±r ve comprehensive ML pipeline structure'a uygun ÅŸekilde tasarlanmÄ±ÅŸtÄ±r.\n",
    "\n",
    "## ğŸ“‹ Project Structure Integration\n",
    "\n",
    "Bu notebook aÅŸaÄŸÄ±daki project structure'Ä±n bir parÃ§asÄ±dÄ±r:\n",
    "\n",
    "```\n",
    "kddcup/\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ raw/                     # âœ… Raw dataset files\n",
    "â”‚   â”œâ”€â”€ processed/               # âœ… Clean data for each task  \n",
    "â”‚   â””â”€â”€ features/                # âœ… Generated features\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ config/config.py         # âœ… Configuration settings\n",
    "â”‚   â”œâ”€â”€ data/data_loader.py      # âš¡ Data loading pipeline\n",
    "â”‚   â”œâ”€â”€ features/base_features.py # âš¡ Feature engineering\n",
    "â”‚   â”œâ”€â”€ models/lgb_ranker.py     # âš¡ LightGBM models\n",
    "â”‚   â””â”€â”€ evaluation/metrics.py    # âš¡ Evaluation metrics\n",
    "â”œâ”€â”€ notebooks/                   # ğŸ““ Current location\n",
    "â”œâ”€â”€ experiments/                 # ğŸ§ª Experiment tracking\n",
    "â””â”€â”€ results/                     # ğŸ“Š Model outputs\n",
    "```\n",
    "\n",
    "## ğŸ¯ Preprocessing Pipeline\n",
    "\n",
    "Bu notebook ÅŸu adÄ±mlarÄ± kapsar:\n",
    "\n",
    "1. **Data Loading & Merging**: KDD Cup specification'a uygun data loading\n",
    "2. **Data Cleaning**: Missing values, duplicates ve validation\n",
    "3. **Feature Engineering**: Basic text features ve similarity metrics\n",
    "4. **Task-Specific Datasets**: 3 farklÄ± task iÃ§in dataset hazÄ±rlama\n",
    "5. **LightGBM Baseline Model**: Ä°lk model training ve evaluation\n",
    "6. **Quality Checks**: Data validation ve integrity checks\n",
    "7. **Data Export**: Processed data'yÄ± structure'a uygun kaydetme\n",
    "\n",
    "## ğŸ“Š Development Phase\n",
    "\n",
    "**Current Phase**: Data Preprocessing + Baseline Model (Target: ~0.2-0.3 NDCG)\n",
    "\n",
    "**Features Implemented**:\n",
    "- Basic text features (length, word count, unique words)\n",
    "- Query-product similarity (word overlap, Jaccard similarity)\n",
    "- Brand/color matching features\n",
    "- Target encoding (ESCI score mapping)\n",
    "\n",
    "**Next Phases**:\n",
    "- Phase 1: Advanced text features (TF-IDF, embeddings) â†’ Target: 0.4-0.5 NDCG\n",
    "- Phase 2: Deep learning features + ensemble methods â†’ Target: 0.6+ NDCG\n",
    "\n",
    "## ğŸ¯ Outputs\n",
    "\n",
    "Bu notebook Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ktan sonra ÅŸu dosyalar oluÅŸturulacak:\n",
    "\n",
    "**Processed Data**:\n",
    "- `data/processed/task_1/` - Query-Product Ranking dataset\n",
    "- `data/processed/task_2/` - Multi-class Classification dataset  \n",
    "- `data/processed/task_3/` - Substitute Identification dataset\n",
    "\n",
    "**Features**:\n",
    "- `data/processed/feature_metadata.pkl` - Feature definitions\n",
    "- `data/processed/dataset_summary.csv` - Dataset statistics\n",
    "\n",
    "**Model Results**:\n",
    "- LightGBM baseline model performance\n",
    "- Feature importance analysis\n",
    "- NDCG evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93228e",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Text processing libraries\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# System and file operations\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Add src to path for config import\n",
    "sys.path.append('../')\n",
    "from src.config.config import Config\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… KÃ¼tÃ¼phaneler baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "print(f\"ğŸ“ Base directory: {Config.BASE_DIR}\")\n",
    "print(f\"ğŸ“Š Raw data directory: {Config.RAW_DATA_DIR}\")\n",
    "print(f\"ğŸ’¾ Processed data directory: {Config.PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb77ad7",
   "metadata": {},
   "source": [
    "## 2. Load Configuration and Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "Config.create_directories()\n",
    "\n",
    "# Define file paths\n",
    "RAW_DATA_PATH = Config.RAW_DATA_DIR\n",
    "PROCESSED_DATA_PATH = Config.PROCESSED_DATA_DIR\n",
    "\n",
    "# Check if raw data files exist\n",
    "required_files = {\n",
    "    'examples': Config.EXAMPLES_FILE,\n",
    "    'products': Config.PRODUCTS_FILE,\n",
    "    'sources': Config.SOURCES_FILE\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ Veri dosyasÄ± kontrolÃ¼:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "missing_files = []\n",
    "for name, file_path in required_files.items():\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size / (1024*1024)  # MB\n",
    "        print(f\"âœ… {name:10}: {file_path.name} ({file_size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"âŒ {name:10}: {file_path.name} - EKSIK!\")\n",
    "        missing_files.append(file_path)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\nâš ï¸  {len(missing_files)} dosya eksik!\")\n",
    "    print(\"LÃ¼tfen data/raw/ klasÃ¶rÃ¼ne ÅŸu dosyalarÄ± yerleÅŸtirin:\")\n",
    "    for file_path in missing_files:\n",
    "        print(f\"   - {file_path.name}\")\n",
    "    print(\"\\nKDD Cup 2022 dataset'ini indirip yerleÅŸtirmeniz gerekiyor.\")\n",
    "else:\n",
    "    print(f\"\\nğŸ‰ TÃ¼m veri dosyalarÄ± mevcut!\")\n",
    "\n",
    "# Create preprocessing functions\n",
    "def print_section(title):\n",
    "    \"\"\"Helper function to print section headers\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š {title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "def print_dataframe_info(df, name):\n",
    "    \"\"\"Helper function to print dataframe info\"\"\"\n",
    "    print(f\"\\n{name} Dataset Info:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "print(\"\\nâœ… KonfigÃ¼rasyon hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8644d15d",
   "metadata": {},
   "source": [
    "## 3. Load Raw Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceebd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"RAW DATA LOADING\")\n",
    "\n",
    "# Load examples, products and sources - exactly as specified in KDD Cup documentation\n",
    "print(\"ğŸ“¥ Loading datasets using official KDD Cup approach...\")\n",
    "\n",
    "import pandas as pd\n",
    "df_examples = pd.read_parquet(Config.EXAMPLES_FILE)\n",
    "df_products = pd.read_parquet(Config.PRODUCTS_FILE)\n",
    "df_sources = pd.read_csv(Config.SOURCES_FILE)\n",
    "\n",
    "print_dataframe_info(df_examples, \"Examples\")\n",
    "print_dataframe_info(df_products, \"Products\")\n",
    "print_dataframe_info(df_sources, \"Sources\")\n",
    "\n",
    "print(f\"\\nâœ… TÃ¼m veri dosyalarÄ± baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "print(f\"   ğŸ“Š Examples: {len(df_examples):,} satÄ±r\")\n",
    "print(f\"   ğŸ›ï¸  Products: {len(df_products):,} satÄ±r\") \n",
    "print(f\"   ğŸ” Sources: {len(df_sources):,} satÄ±r\")\n",
    "\n",
    "# Merge examples with products - exactly as specified\n",
    "print(\"\\nğŸ”— Merging examples with products...\")\n",
    "df_examples_products = pd.merge( \n",
    "    df_examples, \n",
    "    df_products, \n",
    "    how='left', \n",
    "    left_on=['product_locale','product_id'], \n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "\n",
    "print(f\"Merged dataset shape: {df_examples_products.shape}\")\n",
    "\n",
    "# Check merge success\n",
    "merge_success = df_examples_products['product_title'].notna().sum()\n",
    "merge_total = len(df_examples_products)\n",
    "print(f\"Merge success rate: {merge_success/merge_total*100:.1f}% ({merge_success}/{merge_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a70cd",
   "metadata": {},
   "source": [
    "## 4. Explore Data Structure and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"DATA STRUCTURE EXPLORATION\")\n",
    "\n",
    "# Examples dataset analysis\n",
    "print(\"ğŸ“Š EXAMPLES DATASET\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape: {df_examples.shape}\")\n",
    "print(f\"Columns: {list(df_examples.columns)}\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "display(df_examples.head(3))\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df_examples.dtypes)\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "missing_examples = df_examples.isnull().sum()\n",
    "print(missing_examples[missing_examples > 0])\n",
    "\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in df_examples.columns:\n",
    "    unique_count = df_examples[col].nunique()\n",
    "    print(f\"  {col}: {unique_count:,}\")\n",
    "\n",
    "# ESCI label distribution\n",
    "print(\"\\nğŸ“ˆ ESCI Label Distribution:\")\n",
    "esci_dist = df_examples['esci_label'].value_counts().sort_index()\n",
    "print(esci_dist)\n",
    "\n",
    "# Version and split distribution  \n",
    "print(\"\\nğŸ“‹ Version Distribution:\")\n",
    "print(\"Small version:\", df_examples['small_version'].sum())\n",
    "print(\"Large version:\", df_examples['large_version'].sum())\n",
    "\n",
    "print(\"\\nğŸ“‹ Split Distribution:\")\n",
    "print(df_examples['split'].value_counts())\n",
    "\n",
    "# Product locale distribution\n",
    "print(\"\\nğŸŒ Product Locale Distribution:\")\n",
    "locale_dist = df_examples['product_locale'].value_counts()\n",
    "print(locale_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f882353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products dataset analysis\n",
    "print(\"\\n\\nğŸ›ï¸ PRODUCTS DATASET\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape: {df_products.shape}\")\n",
    "print(f\"Columns: {list(df_products.columns)}\")\n",
    "\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "display(df_products.head(3))\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "missing_products = df_products.isnull().sum()\n",
    "print(missing_products[missing_products > 0])\n",
    "\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in df_products.columns:\n",
    "    unique_count = df_products[col].nunique()\n",
    "    print(f\"  {col}: {unique_count:,}\")\n",
    "\n",
    "# Text field statistics\n",
    "text_fields = ['product_title', 'product_description', 'product_bullet_point']\n",
    "print(f\"\\nğŸ“ Text Field Statistics:\")\n",
    "for field in text_fields:\n",
    "    if field in df_products.columns:\n",
    "        non_null = df_products[field].notna().sum()\n",
    "        avg_length = df_products[field].str.len().mean()\n",
    "        print(f\"  {field}:\")\n",
    "        print(f\"    Non-null: {non_null:,} ({non_null/len(df_products)*100:.1f}%)\")\n",
    "        print(f\"    Avg length: {avg_length:.1f} chars\")\n",
    "\n",
    "# Sources dataset analysis  \n",
    "print(\"\\n\\nğŸ” SOURCES DATASET\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape: {df_sources.shape}\")\n",
    "print(f\"Columns: {list(df_sources.columns)}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df_sources.head())\n",
    "\n",
    "print(\"\\nSource distribution:\")\n",
    "if 'source' in df_sources.columns:\n",
    "    source_dist = df_sources['source'].value_counts()\n",
    "    print(source_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c71b3f",
   "metadata": {},
   "source": [
    "## 5. Clean and Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"DATA CLEANING AND VALIDATION\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text data\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and strip\n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def validate_esci_labels(df):\n",
    "    \"\"\"Validate ESCI labels\"\"\"\n",
    "    valid_labels = {'E', 'S', 'C', 'I'}\n",
    "    invalid_labels = set(df['esci_label'].unique()) - valid_labels\n",
    "    \n",
    "    if invalid_labels:\n",
    "        print(f\"âš ï¸  Invalid ESCI labels found: {invalid_labels}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"âœ… All ESCI labels are valid\")\n",
    "        return True\n",
    "\n",
    "# Clean examples dataset\n",
    "print(\"ğŸ§¹ Examples dataset temizleniyor...\")\n",
    "df_examples_clean = df_examples.copy()\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_examples_clean.duplicated().sum()\n",
    "print(f\"Duplicates found: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df_examples_clean = df_examples_clean.drop_duplicates()\n",
    "    print(f\"âœ… {duplicates} duplicate removed\")\n",
    "\n",
    "# Validate ESCI labels\n",
    "validate_esci_labels(df_examples_clean)\n",
    "\n",
    "# Clean query text\n",
    "df_examples_clean['query'] = df_examples_clean['query'].apply(clean_text)\n",
    "\n",
    "# Remove empty queries\n",
    "empty_queries = df_examples_clean['query'].str.len() == 0\n",
    "if empty_queries.sum() > 0:\n",
    "    print(f\"âš ï¸  {empty_queries.sum()} empty queries found, removing...\")\n",
    "    df_examples_clean = df_examples_clean[~empty_queries]\n",
    "\n",
    "print(f\"Examples dataset: {len(df_examples)} â†’ {len(df_examples_clean)} rows\")\n",
    "\n",
    "# Clean products dataset\n",
    "print(\"\\nğŸ§¹ Products dataset temizleniyor...\")\n",
    "df_products_clean = df_products.copy()\n",
    "\n",
    "# Check for duplicates  \n",
    "duplicates = df_products_clean.duplicated(subset=['product_id', 'product_locale']).sum()\n",
    "print(f\"Product duplicates found: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df_products_clean = df_products_clean.drop_duplicates(subset=['product_id', 'product_locale'])\n",
    "    print(f\"âœ… {duplicates} product duplicates removed\")\n",
    "\n",
    "# Clean text fields\n",
    "text_fields = ['product_title', 'product_description', 'product_bullet_point', 'product_brand']\n",
    "for field in text_fields:\n",
    "    if field in df_products_clean.columns:\n",
    "        df_products_clean[field] = df_products_clean[field].apply(clean_text)\n",
    "\n",
    "# Fill missing text fields with empty string\n",
    "df_products_clean[text_fields] = df_products_clean[text_fields].fillna(\"\")\n",
    "\n",
    "print(f\"Products dataset: {len(df_products)} â†’ {len(df_products_clean)} rows\")\n",
    "\n",
    "# Clean sources dataset\n",
    "print(\"\\nğŸ§¹ Sources dataset temizleniyor...\")\n",
    "df_sources_clean = df_sources.copy()\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_sources_clean.duplicated(subset=['query_id']).sum()\n",
    "print(f\"Source duplicates found: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df_sources_clean = df_sources_clean.drop_duplicates(subset=['query_id'])\n",
    "    print(f\"âœ… {duplicates} source duplicates removed\")\n",
    "\n",
    "print(f\"Sources dataset: {len(df_sources)} â†’ {len(df_sources_clean)} rows\")\n",
    "\n",
    "print(\"\\nâœ… Veri temizleme tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b373e1",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering for Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c148dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"FEATURE ENGINEERING\")\n",
    "\n",
    "# Use the merged dataset from previous step\n",
    "print(\"âš™ï¸ Using merged df_examples_products for feature engineering...\")\n",
    "df_master = df_examples_products.copy()\n",
    "\n",
    "print(f\"Master dataset shape: {df_master.shape}\")\n",
    "\n",
    "def create_basic_text_features(df):\n",
    "    \"\"\"Create basic text features\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    # Query features\n",
    "    features['query_len'] = features['query'].str.len()\n",
    "    features['query_word_count'] = features['query'].str.split().str.len()\n",
    "    features['query_unique_words'] = features['query'].apply(lambda x: len(set(str(x).lower().split())))\n",
    "    \n",
    "    # Product title features\n",
    "    features['title_len'] = features['product_title'].str.len()\n",
    "    features['title_word_count'] = features['product_title'].str.split().str.len()\n",
    "    features['title_unique_words'] = features['product_title'].apply(lambda x: len(set(str(x).lower().split())))\n",
    "    \n",
    "    # Product description features\n",
    "    features['description_len'] = features['product_description'].str.len()\n",
    "    features['description_word_count'] = features['product_description'].str.split().str.len()\n",
    "    \n",
    "    # Brand and color features\n",
    "    features['has_brand'] = (features['product_brand'].str.len() > 0).astype(int)\n",
    "    features['has_color'] = (features['product_color'].str.len() > 0).astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_similarity_features(df):\n",
    "    \"\"\"Create query-product similarity features\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    # Exact matches\n",
    "    features['query_in_title'] = features.apply(\n",
    "        lambda x: 1 if str(x['query']).lower() in str(x['product_title']).lower() else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    features['title_in_query'] = features.apply(\n",
    "        lambda x: 1 if str(x['product_title']).lower() in str(x['query']).lower() else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    # Word overlap features\n",
    "    def word_overlap_ratio(text1, text2):\n",
    "        words1 = set(str(text1).lower().split())\n",
    "        words2 = set(str(text2).lower().split())\n",
    "        if len(words1) == 0 or len(words2) == 0:\n",
    "            return 0\n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        return intersection / union if union > 0 else 0\n",
    "    \n",
    "    def word_jaccard_similarity(text1, text2):\n",
    "        words1 = set(str(text1).lower().split())\n",
    "        words2 = set(str(text2).lower().split())\n",
    "        if len(words1) == 0 and len(words2) == 0:\n",
    "            return 1\n",
    "        if len(words1) == 0 or len(words2) == 0:\n",
    "            return 0\n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        return intersection / union\n",
    "    \n",
    "    print(\"ğŸ“Š Similarity features hesaplanÄ±yor...\")\n",
    "    features['query_title_word_overlap'] = features.apply(\n",
    "        lambda x: word_overlap_ratio(x['query'], x['product_title']), axis=1\n",
    "    )\n",
    "    \n",
    "    features['query_title_jaccard'] = features.apply(\n",
    "        lambda x: word_jaccard_similarity(x['query'], x['product_title']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Brand matching\n",
    "    features['brand_in_query'] = features.apply(\n",
    "        lambda x: 1 if str(x['product_brand']).lower() in str(x['query']).lower() and len(str(x['product_brand'])) > 0 else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create features\n",
    "print(\"âš™ï¸ Basic text features oluÅŸturuluyor...\")\n",
    "df_master_features = create_basic_text_features(df_master)\n",
    "\n",
    "print(\"âš™ï¸ Similarity features oluÅŸturuluyor...\")\n",
    "df_master_features = create_similarity_features(df_master_features)\n",
    "\n",
    "# Add ESCI label encoding\n",
    "esci_mapping = Config.ESCI_MAPPING\n",
    "df_master_features['esci_score'] = df_master_features['esci_label'].map(esci_mapping)\n",
    "\n",
    "print(f\"\\nâœ… Feature engineering tamamlandÄ±!\")\n",
    "print(f\"Final dataset shape: {df_master_features.shape}\")\n",
    "\n",
    "# Show feature summary\n",
    "feature_cols = [col for col in df_master_features.columns if col.endswith(('_len', '_count', '_words', '_overlap', '_jaccard', '_in_', 'has_'))]\n",
    "print(f\"Created {len(feature_cols)} features:\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    if i % 3 == 0:\n",
    "        print()\n",
    "    print(f\"  {col:25}\", end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d0d5b",
   "metadata": {},
   "source": [
    "## 7. Create Task-Specific Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"TASK-SPECIFIC DATASETS\")\n",
    "\n",
    "# Filter for English locale only\n",
    "print(\"ğŸŒ Filtering for English locale (US)...\")\n",
    "df_english = df_master_features[df_master_features['product_locale'] == Config.LANGUAGE].copy()\n",
    "print(f\"English dataset shape: {df_english.shape}\")\n",
    "\n",
    "# Task 1: Query-Product Ranking (small version)\n",
    "print(\"\\nğŸ¯ Task 1: Query-Product Ranking\")\n",
    "print(\"-\" * 40)\n",
    "df_task1 = df_english[df_english['small_version'] == 1].copy()\n",
    "\n",
    "df_task1_train = df_task1[df_task1['split'] == 'train'].copy()\n",
    "df_task1_test = df_task1[df_task1['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"Task 1 Total: {len(df_task1):,} examples\")\n",
    "print(f\"  Train: {len(df_task1_train):,} examples\")\n",
    "print(f\"  Test:  {len(df_task1_test):,} examples\")\n",
    "print(f\"  Unique queries: {df_task1['query_id'].nunique():,}\")\n",
    "\n",
    "# ESCI distribution for Task 1\n",
    "task1_esci = df_task1['esci_label'].value_counts().sort_index()\n",
    "print(\"  ESCI distribution:\")\n",
    "for label, count in task1_esci.items():\n",
    "    print(f\"    {label}: {count:,} ({count/len(df_task1)*100:.1f}%)\")\n",
    "\n",
    "# Task 2: Multi-class Product Classification (large version)\n",
    "print(\"\\nğŸ¯ Task 2: Multi-class Product Classification\")\n",
    "print(\"-\" * 50)\n",
    "df_task2 = df_english[df_english['large_version'] == 1].copy()\n",
    "\n",
    "df_task2_train = df_task2[df_task2['split'] == 'train'].copy()\n",
    "df_task2_test = df_task2[df_task2['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"Task 2 Total: {len(df_task2):,} examples\")\n",
    "print(f\"  Train: {len(df_task2_train):,} examples\")\n",
    "print(f\"  Test:  {len(df_task2_test):,} examples\")\n",
    "print(f\"  Unique queries: {df_task2['query_id'].nunique():,}\")\n",
    "\n",
    "# ESCI distribution for Task 2\n",
    "task2_esci = df_task2['esci_label'].value_counts().sort_index()\n",
    "print(\"  ESCI distribution:\")\n",
    "for label, count in task2_esci.items():\n",
    "    print(f\"    {label}: {count:,} ({count/len(df_task2)*100:.1f}%)\")\n",
    "\n",
    "# Task 3: Product Substitute Identification (large version)\n",
    "print(\"\\nğŸ¯ Task 3: Product Substitute Identification\")\n",
    "print(\"-\" * 45)\n",
    "df_task3 = df_english[df_english['large_version'] == 1].copy()\n",
    "\n",
    "# Create substitute label (1 if S, 0 otherwise)\n",
    "df_task3['substitute_label'] = (df_task3['esci_label'] == 'S').astype(int)\n",
    "\n",
    "df_task3_train = df_task3[df_task3['split'] == 'train'].copy()\n",
    "df_task3_test = df_task3[df_task3['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"Task 3 Total: {len(df_task3):,} examples\")\n",
    "print(f\"  Train: {len(df_task3_train):,} examples\")\n",
    "print(f\"  Test:  {len(df_task3_test):,} examples\")\n",
    "print(f\"  Unique queries: {df_task3['query_id'].nunique():,}\")\n",
    "\n",
    "# Substitute distribution for Task 3\n",
    "substitute_dist = df_task3['substitute_label'].value_counts()\n",
    "print(\"  Substitute distribution:\")\n",
    "print(f\"    Non-Substitute (0): {substitute_dist[0]:,} ({substitute_dist[0]/len(df_task3)*100:.1f}%)\")\n",
    "print(f\"    Substitute (1):     {substitute_dist[1]:,} ({substitute_dist[1]/len(df_task3)*100:.1f}%)\")\n",
    "\n",
    "# Create task datasets dictionary for easy access\n",
    "task_datasets = {\n",
    "    'task1': {\n",
    "        'train': df_task1_train,\n",
    "        'test': df_task1_test,\n",
    "        'full': df_task1,\n",
    "        'target_col': 'esci_score',\n",
    "        'description': 'Query-Product Ranking (Small Version)'\n",
    "    },\n",
    "    'task2': {\n",
    "        'train': df_task2_train,\n",
    "        'test': df_task2_test,\n",
    "        'full': df_task2,\n",
    "        'target_col': 'esci_label',\n",
    "        'description': 'Multi-class Product Classification (Large Version)'\n",
    "    },\n",
    "    'task3': {\n",
    "        'train': df_task3_train,\n",
    "        'test': df_task3_test,\n",
    "        'full': df_task3,\n",
    "        'target_col': 'substitute_label',\n",
    "        'description': 'Product Substitute Identification (Large Version)'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… Task-specific datasets hazÄ±rlandÄ±!\")\n",
    "print(f\"   ğŸ“Š Task 1 (Ranking): {len(df_task1):,} examples\")\n",
    "print(f\"   ğŸ¯ Task 2 (Classification): {len(df_task2):,} examples\")\n",
    "print(f\"   ğŸ” Task 3 (Substitute): {len(df_task3):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDD Cup Official Task Preparation (exactly as specified)\n",
    "print_section(\"KDD CUP OFFICIAL TASK PREPARATION\")\n",
    "\n",
    "# Filter and prepare for Task 1 - exactly as specified\n",
    "print(\"ğŸ¯ Filter and prepare for Task 1\")\n",
    "df_task_1 = df_examples_products[df_examples_products[\"small_version\"] == 1]\n",
    "df_task_1_train = df_task_1[df_task_1[\"split\"] == \"train\"]\n",
    "df_task_1_test = df_task_1[df_task_1[\"split\"] == \"test\"]\n",
    "\n",
    "print(f\"Task 1 (official):\")\n",
    "print(f\"  Total: {len(df_task_1):,}\")\n",
    "print(f\"  Train: {len(df_task_1_train):,}\")\n",
    "print(f\"  Test:  {len(df_task_1_test):,}\")\n",
    "\n",
    "# Filter and prepare data for Task 2 - exactly as specified\n",
    "print(\"\\nğŸ¯ Filter and prepare data for Task 2\")\n",
    "df_task_2 = df_examples_products[df_examples_products[\"large_version\"] == 1]\n",
    "df_task_2_train = df_task_2[df_task_2[\"split\"] == \"train\"]\n",
    "df_task_2_test = df_task_2[df_task_2[\"split\"] == \"test\"]\n",
    "\n",
    "print(f\"Task 2 (official):\")\n",
    "print(f\"  Total: {len(df_task_2):,}\")\n",
    "print(f\"  Train: {len(df_task_2_train):,}\")\n",
    "print(f\"  Test:  {len(df_task_2_test):,}\")\n",
    "\n",
    "# Filter and prepare data for Task 3 - exactly as specified\n",
    "print(\"\\nğŸ¯ Filter and prepare data for Task 3\")\n",
    "df_task_3 = df_examples_products[df_examples_products[\"large_version\"] == 1]\n",
    "df_task_3[\"substitute_label\"] = df_task_3[\"esci_label\"].apply(lambda esci_label: 1 if esci_label == \"S\" else 0)\n",
    "# Note: keeping esci_label column (not deleting as in original spec)\n",
    "df_task_3_train = df_task_3[df_task_3[\"split\"] == \"train\"]\n",
    "df_task_3_test = df_task_3[df_task_3[\"split\"] == \"test\"]\n",
    "\n",
    "print(f\"Task 3 (official):\")\n",
    "print(f\"  Total: {len(df_task_3):,}\")\n",
    "print(f\"  Train: {len(df_task_3_train):,}\")\n",
    "print(f\"  Test:  {len(df_task_3_test):,}\")\n",
    "\n",
    "# Optional: Merge queries with sources (as specified)\n",
    "print(\"\\nğŸ”— Merge queries with sources (optional)\")\n",
    "df_examples_products_source = pd.merge( \n",
    "    df_examples_products, \n",
    "    df_sources, \n",
    "    how='left', \n",
    "    left_on=['query_id'],\n",
    "    right_on=['query_id']\n",
    ")\n",
    "\n",
    "print(f\"With sources shape: {df_examples_products_source.shape}\")\n",
    "\n",
    "print(f\"\\nâœ… KDD Cup official task preparation tamamlandÄ±!\")\n",
    "print(f\"   ğŸ“Š df_task_1: {len(df_task_1):,} examples\")\n",
    "print(f\"   ğŸ¯ df_task_2: {len(df_task_2):,} examples\")\n",
    "print(f\"   ğŸ” df_task_3: {len(df_task_3):,} examples\")\n",
    "print(f\"   ğŸ“‹ df_examples_products_source: {len(df_examples_products_source):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5acdd3d",
   "metadata": {},
   "source": [
    "## 7.5. LightGBM Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"LIGHTGBM MODEL TRAINING\")\n",
    "\n",
    "# Import LightGBM and additional libraries\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "print(\"ğŸ“¦ LightGBM ve metric kÃ¼tÃ¼phaneleri yÃ¼klendi!\")\n",
    "\n",
    "# Prepare feature-enhanced data for Task 1 (using English subset with features)\n",
    "print(\"\\nğŸ¯ Task 1 iÃ§in feature-enhanced dataset hazÄ±rlanÄ±yor...\")\n",
    "df_task1_features = df_english[df_english['small_version'] == 1].copy()\n",
    "\n",
    "print(f\"Task 1 dataset shape: {df_task1_features.shape}\")\n",
    "print(f\"Feature columns available: {len([col for col in df_task1_features.columns if col.endswith(('_len', '_count', '_words', '_overlap', '_jaccard', '_in_', 'has_'))])}\")\n",
    "\n",
    "# Split train/test\n",
    "train_data = df_task1_features[df_task1_features['split'] == 'train'].copy()\n",
    "test_data = df_task1_features[df_task1_features['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"Train size: {len(train_data):,}\")\n",
    "print(f\"Test size: {len(test_data):,}\")\n",
    "print(f\"Train queries: {train_data['query_id'].nunique():,}\")\n",
    "print(f\"Test queries: {test_data['query_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d751eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature preparation\n",
    "print(\"\\nâš™ï¸ Feature hazÄ±rlÄ±ÄŸÄ± yapÄ±lÄ±yor...\")\n",
    "\n",
    "# Select feature columns that we created\n",
    "feature_cols = [col for col in train_data.columns if col.endswith(('_len', '_count', '_words', '_overlap', '_jaccard', '_in_', 'has_'))]\n",
    "print(f\"Selected features: {len(feature_cols)}\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    if i % 3 == 0:\n",
    "        print()\n",
    "    print(f\"  {col:25}\", end=\"\")\n",
    "print()\n",
    "\n",
    "# Prepare features and target\n",
    "X_train = train_data[feature_cols].fillna(0)\n",
    "y_train = train_data['esci_score']\n",
    "X_test = test_data[feature_cols].fillna(0)\n",
    "y_test = test_data['esci_score']\n",
    "\n",
    "print(f\"\\nFeatures shape: {X_train.shape}\")\n",
    "print(f\"Target distribution in train:\")\n",
    "print(train_data['esci_label'].value_counts().sort_index())\n",
    "\n",
    "# Prepare ranking data (group by query_id for LightGBM ranker)\n",
    "print(\"\\nğŸ“Š Ranking data hazÄ±rlanÄ±yor...\")\n",
    "train_sorted = train_data.sort_values('query_id').reset_index(drop=True)\n",
    "test_sorted = test_data.sort_values('query_id').reset_index(drop=True)\n",
    "\n",
    "# Get features for sorted data\n",
    "X_train_sorted = train_sorted[feature_cols].fillna(0)\n",
    "y_train_sorted = train_sorted['esci_score']\n",
    "X_test_sorted = test_sorted[feature_cols].fillna(0)\n",
    "y_test_sorted = test_sorted['esci_score']\n",
    "\n",
    "# Create group information (number of items per query)\n",
    "train_groups = train_sorted.groupby('query_id').size().values\n",
    "test_groups = test_sorted.groupby('query_id').size().values\n",
    "\n",
    "print(f\"Train groups: {len(train_groups)} queries\")\n",
    "print(f\"Test groups: {len(test_groups)} queries\")\n",
    "print(f\"Group sizes - Train: min={min(train_groups)}, max={max(train_groups)}, mean={np.mean(train_groups):.1f}\")\n",
    "print(f\"Group sizes - Test: min={min(test_groups)}, max={max(test_groups)}, mean={np.mean(test_groups):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM Ranker\n",
    "print(\"\\nğŸš€ LightGBM Ranker eÄŸitimi baÅŸlatÄ±lÄ±yor...\")\n",
    "\n",
    "# Create train/validation split (query-level split to avoid leakage)\n",
    "unique_train_queries = train_sorted['query_id'].unique()\n",
    "val_queries, train_queries_final = train_test_split(unique_train_queries, test_size=0.8, random_state=42)\n",
    "\n",
    "# Create validation mask\n",
    "val_mask = train_sorted['query_id'].isin(val_queries)\n",
    "train_final_mask = train_sorted['query_id'].isin(train_queries_final)\n",
    "\n",
    "# Split data\n",
    "X_train_final = X_train_sorted[train_final_mask]\n",
    "y_train_final = y_train_sorted[train_final_mask]\n",
    "train_groups_final = train_sorted[train_final_mask].groupby('query_id').size().values\n",
    "\n",
    "X_val = X_train_sorted[val_mask]\n",
    "y_val = y_train_sorted[val_mask]\n",
    "val_groups = train_sorted[val_mask].groupby('query_id').size().values\n",
    "\n",
    "print(f\"Final train: {len(X_train_final)} samples, {len(train_groups_final)} queries\")\n",
    "print(f\"Validation: {len(X_val)} samples, {len(val_groups)} queries\")\n",
    "\n",
    "# LightGBM parameters for ranking\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ LightGBM parametreleri:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nğŸ“Š LightGBM datasets oluÅŸturuluyor...\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_dataset = lgb.Dataset(\n",
    "    X_train_final, \n",
    "    label=y_train_final, \n",
    "    group=train_groups_final\n",
    ")\n",
    "\n",
    "val_dataset = lgb.Dataset(\n",
    "    X_val, \n",
    "    label=y_val, \n",
    "    group=val_groups,\n",
    "    reference=train_dataset\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nğŸ¯ Model eÄŸitimi baÅŸlÄ±yor...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_dataset,\n",
    "    valid_sets=[val_dataset],\n",
    "    num_boost_round=500,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=25)\n",
    "    ]\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nâ±ï¸ EÄŸitim sÃ¼resi: {training_time:.2f} saniye\")\n",
    "print(f\"ğŸ† Best iteration: {model.best_iteration}\")\n",
    "print(f\"ğŸ“Š Best score: {model.best_score['valid_0']['ndcg@10']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d70cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "print_section(\"MODEL EVALUATION\")\n",
    "\n",
    "# Predict on test data\n",
    "print(\"ğŸ“Š Test seti Ã¼zerinde tahmin yapÄ±lÄ±yor...\")\n",
    "y_pred_test = model.predict(X_test_sorted, num_iteration=model.best_iteration)\n",
    "\n",
    "# Calculate NDCG scores for different k values\n",
    "def calculate_ndcg_by_query(y_true, y_pred, test_groups, k_values=[1, 5, 10]):\n",
    "    \"\"\"Calculate NDCG@k for each query and return average\"\"\"\n",
    "    ndcg_scores = {f'ndcg@{k}': [] for k in k_values}\n",
    "    \n",
    "    start_idx = 0\n",
    "    for group_size in test_groups:\n",
    "        end_idx = start_idx + group_size\n",
    "        \n",
    "        # Get true and predicted relevance for this query\n",
    "        y_true_query = y_true[start_idx:end_idx].values\n",
    "        y_pred_query = y_pred[start_idx:end_idx]\n",
    "        \n",
    "        # Calculate NDCG@k for this query\n",
    "        for k in k_values:\n",
    "            if len(y_true_query) >= k:\n",
    "                ndcg_k = ndcg_score([y_true_query], [y_pred_query], k=k)\n",
    "                ndcg_scores[f'ndcg@{k}'].append(ndcg_k)\n",
    "        \n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Calculate average NDCG@k\n",
    "    avg_ndcg = {}\n",
    "    for metric, scores in ndcg_scores.items():\n",
    "        avg_ndcg[metric] = np.mean(scores) if scores else 0.0\n",
    "    \n",
    "    return avg_ndcg\n",
    "\n",
    "# Calculate test performance\n",
    "test_ndcg = calculate_ndcg_by_query(y_test_sorted, y_pred_test, test_groups)\n",
    "\n",
    "print(\"ğŸ“ˆ Test Set Performance:\")\n",
    "print(\"-\" * 30)\n",
    "for metric, score in test_ndcg.items():\n",
    "    print(f\"  {metric.upper()}: {score:.4f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\nğŸ¯ Feature Importance Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "feature_importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = feature_cols\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
    "    print(f\"  {i+1:2d}. {row['feature']:25} {row['importance']:8.0f}\")\n",
    "\n",
    "# Performance by ESCI label\n",
    "print(\"\\nğŸ“Š Performance by ESCI Label:\")\n",
    "print(\"-\" * 35)\n",
    "test_sorted_with_pred = test_sorted.copy()\n",
    "test_sorted_with_pred['pred_score'] = y_pred_test\n",
    "\n",
    "esci_performance = {}\n",
    "for label in ['E', 'S', 'C', 'I']:\n",
    "    label_mask = test_sorted_with_pred['esci_label'] == label\n",
    "    if label_mask.sum() > 0:\n",
    "        label_data = test_sorted_with_pred[label_mask]\n",
    "        avg_true_score = label_data['esci_score'].mean()\n",
    "        avg_pred_score = label_data['pred_score'].mean()\n",
    "        count = len(label_data)\n",
    "        esci_performance[label] = {\n",
    "            'count': count,\n",
    "            'avg_true': avg_true_score,\n",
    "            'avg_pred': avg_pred_score\n",
    "        }\n",
    "        print(f\"  {label} ({count:,} samples): True={avg_true_score:.3f}, Pred={avg_pred_score:.3f}\")\n",
    "\n",
    "# Query-level analysis\n",
    "print(\"\\nğŸ” Query-level Analysis:\")\n",
    "print(\"-\" * 25)\n",
    "query_stats = test_sorted.groupby('query_id').agg({\n",
    "    'esci_score': ['count', 'mean', 'std'],\n",
    "    'query': 'first'\n",
    "}).round(3)\n",
    "\n",
    "query_stats.columns = ['query_size', 'avg_relevance', 'relevance_std', 'query_text']\n",
    "print(f\"Average query size: {query_stats['query_size'].mean():.1f}\")\n",
    "print(f\"Query size range: {query_stats['query_size'].min()}-{query_stats['query_size'].max()}\")\n",
    "print(f\"Average relevance: {query_stats['avg_relevance'].mean():.3f}\")\n",
    "\n",
    "# Best and worst performing queries\n",
    "print(\"\\nTop 5 queries by average relevance:\")\n",
    "top_queries = query_stats.nlargest(5, 'avg_relevance')\n",
    "for i, (query_id, row) in enumerate(top_queries.iterrows()):\n",
    "    print(f\"  {i+1}. '{row['query_text'][:50]}...' (size: {row['query_size']}, avg: {row['avg_relevance']:.3f})\")\n",
    "\n",
    "print(f\"\\nâœ… Model evaluation tamamlandÄ±!\")\n",
    "print(f\"ğŸ¯ Final NDCG@10: {test_ndcg['ndcg@10']:.4f}\")\n",
    "print(f\"â±ï¸ Total processing time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd85c00",
   "metadata": {},
   "source": [
    "## 8. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ce053",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"DATA QUALITY CHECKS\")\n",
    "\n",
    "def perform_quality_checks(task_name, task_data):\n",
    "    \"\"\"Perform comprehensive quality checks on task data\"\"\"\n",
    "    print(f\"\\nğŸ” {task_name} Quality Checks\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    train_df = task_data['train']\n",
    "    test_df = task_data['test']\n",
    "    target_col = task_data['target_col']\n",
    "    \n",
    "    # Basic checks\n",
    "    print(f\"âœ… Train shape: {train_df.shape}\")\n",
    "    print(f\"âœ… Test shape: {test_df.shape}\")\n",
    "    \n",
    "    # Missing values check\n",
    "    train_missing = train_df.isnull().sum().sum()\n",
    "    test_missing = test_df.isnull().sum().sum()\n",
    "    print(f\"âœ… Train missing values: {train_missing}\")\n",
    "    print(f\"âœ… Test missing values: {test_missing}\")\n",
    "    \n",
    "    # Target distribution check\n",
    "    if target_col in train_df.columns:\n",
    "        target_dist = train_df[target_col].value_counts().sort_index()\n",
    "        print(f\"âœ… Target distribution:\")\n",
    "        for value, count in target_dist.items():\n",
    "            print(f\"   {value}: {count:,} ({count/len(train_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Feature completeness\n",
    "    feature_cols = [col for col in train_df.columns if col.endswith(('_len', '_count', '_words', '_overlap', '_jaccard', '_in_', 'has_'))]\n",
    "    feature_missing = train_df[feature_cols].isnull().sum().sum()\n",
    "    print(f\"âœ… Feature columns: {len(feature_cols)}\")\n",
    "    print(f\"âœ… Feature missing values: {feature_missing}\")\n",
    "    \n",
    "    # Query overlap check (no query should be in both train and test)\n",
    "    train_queries = set(train_df['query_id'].unique())\n",
    "    test_queries = set(test_df['query_id'].unique())\n",
    "    query_overlap = len(train_queries.intersection(test_queries))\n",
    "    \n",
    "    if query_overlap == 0:\n",
    "        print(f\"âœ… No query overlap between train/test\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Query overlap detected: {query_overlap} queries\")\n",
    "    \n",
    "    print(f\"âœ… Train unique queries: {len(train_queries):,}\")\n",
    "    print(f\"âœ… Test unique queries: {len(test_queries):,}\")\n",
    "    \n",
    "    return {\n",
    "        'train_shape': train_df.shape,\n",
    "        'test_shape': test_df.shape,\n",
    "        'train_missing': train_missing,\n",
    "        'test_missing': test_missing,\n",
    "        'feature_count': len(feature_cols),\n",
    "        'query_overlap': query_overlap\n",
    "    }\n",
    "\n",
    "# Perform quality checks for all tasks\n",
    "quality_results = {}\n",
    "\n",
    "for task_id, task_data in task_datasets.items():\n",
    "    task_name = f\"Task {task_id[-1]}: {task_data['description']}\"\n",
    "    quality_results[task_id] = perform_quality_checks(task_name, task_data)\n",
    "\n",
    "# Summary of all checks\n",
    "print(f\"\\n\\nğŸ“‹ QUALITY CHECK SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_checks_passed = True\n",
    "for task_id, results in quality_results.items():\n",
    "    task_num = task_id[-1]\n",
    "    status = \"âœ… PASSED\" if results['query_overlap'] == 0 and results['train_missing'] == 0 else \"âš ï¸  WARNING\"\n",
    "    \n",
    "    if results['query_overlap'] > 0 or results['train_missing'] > 0:\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(f\"Task {task_num}: {status}\")\n",
    "    print(f\"  Train: {results['train_shape'][0]:,} rows, {results['train_shape'][1]} cols\")\n",
    "    print(f\"  Test:  {results['test_shape'][0]:,} rows, {results['test_shape'][1]} cols\")\n",
    "    print(f\"  Features: {results['feature_count']}\")\n",
    "    print(f\"  Missing: {results['train_missing']} (train), {results['test_missing']} (test)\")\n",
    "    print(f\"  Query overlap: {results['query_overlap']}\")\n",
    "    print()\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(\"ğŸ‰ All quality checks passed! Data is ready for training.\")\n",
    "else:\n",
    "    print(\"âš ï¸  Some quality issues detected. Please review before training.\")\n",
    "\n",
    "# Create feature list for reference\n",
    "feature_columns = [col for col in df_task1.columns if col.endswith(('_len', '_count', '_words', '_overlap', '_jaccard', '_in_', 'has_'))]\n",
    "print(f\"\\nğŸ“Š Available Features ({len(feature_columns)}):\")\n",
    "for i, col in enumerate(sorted(feature_columns)):\n",
    "    if i % 2 == 0:\n",
    "        print()\n",
    "    print(f\"  {col:35}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0496942",
   "metadata": {},
   "source": [
    "## 9. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"SAVING PROCESSED DATA\")\n",
    "\n",
    "# Create output directories\n",
    "output_dirs = {\n",
    "    'task1': Config.PROCESSED_DATA_DIR / \"task_1\",\n",
    "    'task2': Config.PROCESSED_DATA_DIR / \"task_2\", \n",
    "    'task3': Config.PROCESSED_DATA_DIR / \"task_3\"\n",
    "}\n",
    "\n",
    "for task_dir in output_dirs.values():\n",
    "    task_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "print(\"ğŸ’¾ Saving processed datasets...\")\n",
    "\n",
    "saved_files = []\n",
    "\n",
    "for task_id, task_data in task_datasets.items():\n",
    "    task_dir = output_dirs[task_id]\n",
    "    task_num = task_id[-1]\n",
    "    \n",
    "    # Save train and test sets\n",
    "    train_file = task_dir / f\"train.parquet\"\n",
    "    test_file = task_dir / f\"test.parquet\"\n",
    "    full_file = task_dir / f\"full.parquet\"\n",
    "    \n",
    "    task_data['train'].to_parquet(train_file, index=False)\n",
    "    task_data['test'].to_parquet(test_file, index=False) \n",
    "    task_data['full'].to_parquet(full_file, index=False)\n",
    "    \n",
    "    saved_files.extend([train_file, test_file, full_file])\n",
    "    \n",
    "    print(f\"âœ… Task {task_num} saved:\")\n",
    "    print(f\"   ğŸ“ {train_file.relative_to(Config.BASE_DIR)}\")\n",
    "    print(f\"   ğŸ“ {test_file.relative_to(Config.BASE_DIR)}\")\n",
    "    print(f\"   ğŸ“ {full_file.relative_to(Config.BASE_DIR)}\")\n",
    "\n",
    "# Save feature metadata\n",
    "feature_metadata = {\n",
    "    'feature_columns': feature_columns,\n",
    "    'esci_mapping': Config.ESCI_MAPPING,\n",
    "    'text_fields': ['query', 'product_title', 'product_description', 'product_bullet_point', 'product_brand'],\n",
    "    'created_features': {\n",
    "        'basic_text': [col for col in feature_columns if col.endswith(('_len', '_count', '_words'))],\n",
    "        'similarity': [col for col in feature_columns if col.endswith(('_overlap', '_jaccard', '_in_'))],\n",
    "        'categorical': [col for col in feature_columns if col.startswith('has_')]\n",
    "    },\n",
    "    'preprocessing_info': {\n",
    "        'total_examples': len(df_examples),\n",
    "        'english_examples': len(df_english),\n",
    "        'feature_count': len(feature_columns),\n",
    "        'quality_passed': all_checks_passed\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_file = Config.PROCESSED_DATA_DIR / \"feature_metadata.pkl\"\n",
    "with open(metadata_file, 'wb') as f:\n",
    "    pickle.dump(feature_metadata, f)\n",
    "\n",
    "saved_files.append(metadata_file)\n",
    "print(f\"âœ… Feature metadata saved: {metadata_file.relative_to(Config.BASE_DIR)}\")\n",
    "\n",
    "# Create summary CSV\n",
    "summary_data = []\n",
    "for task_id, task_data in task_datasets.items():\n",
    "    task_num = task_id[-1]\n",
    "    train_df = task_data['train']\n",
    "    test_df = task_data['test']\n",
    "    \n",
    "    summary_data.append({\n",
    "        'task': f\"Task {task_num}\",\n",
    "        'description': task_data['description'],\n",
    "        'train_samples': len(train_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'total_samples': len(task_data['full']),\n",
    "        'unique_queries': task_data['full']['query_id'].nunique(),\n",
    "        'target_column': task_data['target_col'],\n",
    "        'feature_count': len(feature_columns)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_file = Config.PROCESSED_DATA_DIR / \"dataset_summary.csv\"\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "saved_files.append(summary_file)\n",
    "\n",
    "print(f\"âœ… Dataset summary saved: {summary_file.relative_to(Config.BASE_DIR)}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nğŸ“Š DATASET SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "display(summary_df)\n",
    "\n",
    "print(f\"\\nğŸ‰ PREPROCESSING COMPLETED!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"âœ… Processed {len(df_examples):,} original examples\")\n",
    "print(f\"âœ… Created {len(feature_columns)} features\") \n",
    "print(f\"âœ… Generated {len(task_datasets)} task-specific datasets\")\n",
    "print(f\"âœ… Saved {len(saved_files)} files\")\n",
    "print(f\"âœ… Quality checks: {'PASSED' if all_checks_passed else 'WARNINGS'}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Output Directory: {Config.PROCESSED_DATA_DIR}\")\n",
    "print(f\"ğŸ“ Files saved:\")\n",
    "for file_path in saved_files:\n",
    "    file_size = file_path.stat().st_size / (1024*1024)  # MB\n",
    "    print(f\"   {str(file_path.relative_to(Config.BASE_DIR)):50} ({file_size:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nğŸš€ Ready for training! Use the following files:\")\n",
    "print(f\"   Task 1 (Ranking): data/processed/task_1/\")\n",
    "print(f\"   Task 2 (Classification): data/processed/task_2/\")\n",
    "print(f\"   Task 3 (Substitute): data/processed/task_3/\")\n",
    "print(f\"\\nğŸ’¡ Next step: python main.py --task 1\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
