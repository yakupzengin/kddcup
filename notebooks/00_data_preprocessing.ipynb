{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95147947",
   "metadata": {},
   "source": [
    "# KDD Cup 2022 ESCI Challenge - Data Preprocessing\n",
    "\n",
    "Bu notebook, Shopping Queries Dataset'ini preprocessing yapmak iÃ§in kullanÄ±lÄ±r. AÅŸaÄŸÄ±daki adÄ±mlarÄ± iÃ§erir:\n",
    "\n",
    "## ğŸ“‹ Preprocessing Pipeline\n",
    "\n",
    "1. **Veri YÃ¼kleme**: Raw data dosyalarÄ±nÄ±n yÃ¼klenmesi\n",
    "2. **Veri Temizleme**: Missing values, duplicates ve data validation\n",
    "3. **Text Preprocessing**: Query ve product text verilerinin iÅŸlenmesi  \n",
    "4. **Feature Engineering**: Temel text features oluÅŸturma\n",
    "5. **Task-Specific Datasets**: Her task iÃ§in ayrÄ± dataset hazÄ±rlama\n",
    "6. **Data Quality Checks**: Son validation kontrolleri\n",
    "7. **Save Processed Data**: Ä°ÅŸlenmiÅŸ verilerin kaydedilmesi\n",
    "\n",
    "## ğŸ¯ Output\n",
    "\n",
    "Bu notebook Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ktan sonra `data/processed/` klasÃ¶rÃ¼nde hazÄ±r dataset'ler oluÅŸturulacak:\n",
    "- Task 1: Query-Product Ranking dataset\n",
    "- Task 2: Multi-class Classification dataset  \n",
    "- Task 3: Substitute Identification dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93228e",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Text processing libraries\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# System and file operations\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Add src to path for config import\n",
    "sys.path.append('../')\n",
    "from src.config.config import Config\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… KÃ¼tÃ¼phaneler baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "print(f\"ğŸ“ Base directory: {Config.BASE_DIR}\")\n",
    "print(f\"ğŸ“Š Raw data directory: {Config.RAW_DATA_DIR}\")\n",
    "print(f\"ğŸ’¾ Processed data directory: {Config.PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb77ad7",
   "metadata": {},
   "source": [
    "## 2. Load Configuration and Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "Config.create_directories()\n",
    "\n",
    "# Define file paths\n",
    "RAW_DATA_PATH = Config.RAW_DATA_DIR\n",
    "PROCESSED_DATA_PATH = Config.PROCESSED_DATA_DIR\n",
    "\n",
    "# Check if raw data files exist\n",
    "required_files = {\n",
    "    'examples': Config.EXAMPLES_FILE,\n",
    "    'products': Config.PRODUCTS_FILE,\n",
    "    'sources': Config.SOURCES_FILE\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ Veri dosyasÄ± kontrolÃ¼:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "missing_files = []\n",
    "for name, file_path in required_files.items():\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size / (1024*1024)  # MB\n",
    "        print(f\"âœ… {name:10}: {file_path.name} ({file_size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"âŒ {name:10}: {file_path.name} - EKSIK!\")\n",
    "        missing_files.append(file_path)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\nâš ï¸  {len(missing_files)} dosya eksik!\")\n",
    "    print(\"LÃ¼tfen data/raw/ klasÃ¶rÃ¼ne ÅŸu dosyalarÄ± yerleÅŸtirin:\")\n",
    "    for file_path in missing_files:\n",
    "        print(f\"   - {file_path.name}\")\n",
    "    print(\"\\nKDD Cup 2022 dataset'ini indirip yerleÅŸtirmeniz gerekiyor.\")\n",
    "else:\n",
    "    print(f\"\\nğŸ‰ TÃ¼m veri dosyalarÄ± mevcut!\")\n",
    "\n",
    "# Create preprocessing functions\n",
    "def print_section(title):\n",
    "    \"\"\"Helper function to print section headers\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š {title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "def print_dataframe_info(df, name):\n",
    "    \"\"\"Helper function to print dataframe info\"\"\"\n",
    "    print(f\"\\n{name} Dataset Info:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "print(\"\\nâœ… KonfigÃ¼rasyon hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8644d15d",
   "metadata": {},
   "source": [
    "## 3. Load Raw Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceebd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"RAW DATA LOADING\")\n",
    "\n",
    "# Load examples, products and sources - exactly as specified in KDD Cup documentation\n",
    "print(\"ğŸ“¥ Loading datasets using official KDD Cup approach...\")\n",
    "\n",
    "import pandas as pd\n",
    "df_examples = pd.read_parquet(Config.EXAMPLES_FILE)\n",
    "df_products = pd.read_parquet(Config.PRODUCTS_FILE)\n",
    "df_sources = pd.read_csv(Config.SOURCES_FILE)\n",
    "\n",
    "print_dataframe_info(df_examples, \"Examples\")\n",
    "print_dataframe_info(df_products, \"Products\")\n",
    "print_dataframe_info(df_sources, \"Sources\")\n",
    "\n",
    "print(f\"\\nâœ… TÃ¼m veri dosyalarÄ± baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "print(f\"   ğŸ“Š Examples: {len(df_examples):,} satÄ±r\")\n",
    "print(f\"   ğŸ›ï¸  Products: {len(df_products):,} satÄ±r\") \n",
    "print(f\"   ğŸ” Sources: {len(df_sources):,} satÄ±r\")\n",
    "\n",
    "# Merge examples with products - exactly as specified\n",
    "print(\"\\nğŸ”— Merging examples with products...\")\n",
    "df_examples_products = pd.merge( \n",
    "    df_examples, \n",
    "    df_products, \n",
    "    how='left', \n",
    "    left_on=['product_locale','product_id'], \n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "\n",
    "print(f\"Merged dataset shape: {df_examples_products.shape}\")\n",
    "\n",
    "# Check merge success\n",
    "merge_success = df_examples_products['product_title'].notna().sum()\n",
    "merge_total = len(df_examples_products)\n",
    "print(f\"Merge success rate: {merge_success/merge_total*100:.1f}% ({merge_success}/{merge_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a70cd",
   "metadata": {},
   "source": [
    "## 4. Explore Data Structure and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"DATA STRUCTURE EXPLORATION\")\n",
    "\n",
    "# Examples dataset analysis\n",
    "print(\"ğŸ“Š EXAMPLES DATASET\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape: {df_examples.shape}\")\n",
    "print(f\"Columns: {list(df_examples.columns)}\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "display(df_examples.head(3))\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df_examples.dtypes)\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "missing_examples = df_examples.isnull().sum()\n",
    "print(missing_examples[missing_examples > 0])\n",
    "\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in df_examples.columns:\n",
    "    unique_count = df_examples[col].nunique()\n",
    "    print(f\"  {col}: {unique_count:,}\")\n",
    "\n",
    "# ESCI label distribution\n",
    "print(\"\\nğŸ“ˆ ESCI Label Distribution:\")\n",
    "esci_dist = df_examples['esci_label'].value_counts().sort_index()\n",
    "print(esci_dist)\n",
    "\n",
    "# Version and split distribution  \n",
    "print(\"\\nğŸ“‹ Version Distribution:\")\n",
    "print(\"Small version:\", df_examples['small_version'].sum())\n",
    "print(\"Large version:\", df_examples['large_version'].sum())\n",
    "\n",
    "print(\"\\nğŸ“‹ Split Distribution:\")\n",
    "print(df_examples['split'].value_counts())\n",
    "\n",
    "# Product locale distribution\n",
    "print(\"\\nğŸŒ Product Locale Distribution:\")\n",
    "locale_dist = df_examples['product_locale'].value_counts()\n",
    "print(locale_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f882353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products dataset analysis\n",
    "print(\"\\n\\nğŸ›ï¸ PRODUCTS DATASET\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape: {df_products.shape}\")\n",
    "print(f\"Columns: {list(df_products.columns)}\")\n",
    "\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "display(df_products.head(3))\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "missing_products = df_products.isnull().sum()\n",
    "print(missing_products[missing_products > 0])\n",
    "\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in df_products.columns:\n",
    "    unique_count = df_products[col].nunique()\n",
    "    print(f\"  {col}: {unique_count:,}\")\n",
    "\n",
    "# Text field statistics\n",
    "text_fields = ['product_title', 'product_description', 'product_bullet_point']\n",
    "print(f\"\\nğŸ“ Text Field Statistics:\")\n",
    "for field in text_fields:\n",
    "    if field in df_products.columns:\n",
    "        non_null = df_products[field].notna().sum()\n",
    "        avg_length = df_products[field].str.len().mean()\n",
    "        print(f\"  {field}:\")\n",
    "        print(f\"    Non-null: {non_null:,} ({non_null/len(df_products)*100:.1f}%)\")\n",
    "        print(f\"    Avg length: {avg_length:.1f} chars\")\n",
    "\n",
    "# Sources dataset analysis  \n",
    "print(\"\\n\\nğŸ” SOURCES DATASET\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape: {df_sources.shape}\")\n",
    "print(f\"Columns: {list(df_sources.columns)}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df_sources.head())\n",
    "\n",
    "print(\"\\nSource distribution:\")\n",
    "if 'source' in df_sources.columns:\n",
    "    source_dist = df_sources['source'].value_counts()\n",
    "    print(source_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c71b3f",
   "metadata": {},
   "source": [
    "## 5. Clean and Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"DATA CLEANING AND VALIDATION\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text data\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and strip\n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def validate_esci_labels(df):\n",
    "    \"\"\"Validate ESCI labels\"\"\"\n",
    "    valid_labels = {'E', 'S', 'C', 'I'}\n",
    "    invalid_labels = set(df['esci_label'].unique()) - valid_labels\n",
    "    \n",
    "    if invalid_labels:\n",
    "        print(f\"âš ï¸  Invalid ESCI labels found: {invalid_labels}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"âœ… All ESCI labels are valid\")\n",
    "        return True\n",
    "\n",
    "# Clean examples dataset\n",
    "print(\"ğŸ§¹ Examples dataset temizleniyor...\")\n",
    "df_examples_clean = df_examples.copy()\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_examples_clean.duplicated().sum()\n",
    "print(f\"Duplicates found: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df_examples_clean = df_examples_clean.drop_duplicates()\n",
    "    print(f\"âœ… {duplicates} duplicate removed\")\n",
    "\n",
    "# Validate ESCI labels\n",
    "validate_esci_labels(df_examples_clean)\n",
    "\n",
    "# Clean query text\n",
    "df_examples_clean['query'] = df_examples_clean['query'].apply(clean_text)\n",
    "\n",
    "# Remove empty queries\n",
    "empty_queries = df_examples_clean['query'].str.len() == 0\n",
    "if empty_queries.sum() > 0:\n",
    "    print(f\"âš ï¸  {empty_queries.sum()} empty queries found, removing...\")\n",
    "    df_examples_clean = df_examples_clean[~empty_queries]\n",
    "\n",
    "print(f\"Examples dataset: {len(df_examples)} â†’ {len(df_examples_clean)} rows\")\n",
    "\n",
    "# Clean products dataset\n",
    "print(\"\\nğŸ§¹ Products dataset temizleniyor...\")\n",
    "df_products_clean = df_products.copy()\n",
    "\n",
    "# Check for duplicates  \n",
    "duplicates = df_products_clean.duplicated(subset=['product_id', 'product_locale']).sum()\n",
    "print(f\"Product duplicates found: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df_products_clean = df_products_clean.drop_duplicates(subset=['product_id', 'product_locale'])\n",
    "    print(f\"âœ… {duplicates} product duplicates removed\")\n",
    "\n",
    "# Clean text fields\n",
    "text_fields = ['product_title', 'product_description', 'product_bullet_point', 'product_brand']\n",
    "for field in text_fields:\n",
    "    if field in df_products_clean.columns:\n",
    "        df_products_clean[field] = df_products_clean[field].apply(clean_text)\n",
    "\n",
    "# Fill missing text fields with empty string\n",
    "df_products_clean[text_fields] = df_products_clean[text_fields].fillna(\"\")\n",
    "\n",
    "print(f\"Products dataset: {len(df_products)} â†’ {len(df_products_clean)} rows\")\n",
    "\n",
    "# Clean sources dataset\n",
    "print(\"\\nğŸ§¹ Sources dataset temizleniyor...\")\n",
    "df_sources_clean = df_sources.copy()\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_sources_clean.duplicated(subset=['query_id']).sum()\n",
    "print(f\"Source duplicates found: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df_sources_clean = df_sources_clean.drop_duplicates(subset=['query_id'])\n",
    "    print(f\"âœ… {duplicates} source duplicates removed\")\n",
    "\n",
    "print(f\"Sources dataset: {len(df_sources)} â†’ {len(df_sources_clean)} rows\")\n",
    "\n",
    "print(\"\\nâœ… Veri temizleme tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b373e1",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering for Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c148dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"FEATURE ENGINEERING\")\n",
    "\n",
    "# Use the merged dataset from previous step\n",
    "print(\"âš™ï¸ Using merged df_examples_products for feature engineering...\")\n",
    "df_master = df_examples_products.copy()\n",
    "\n",
    "print(f\"Master dataset shape: {df_master.shape}\")\n",
    "\n",
    "def create_basic_text_features(df):\n",
    "    \"\"\"Create basic text features\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    # Query features\n",
    "    features['query_len'] = features['query'].str.len()\n",
    "    features['query_word_count'] = features['query'].str.split().str.len()\n",
    "    features['query_unique_words'] = features['query'].apply(lambda x: len(set(str(x).lower().split())))\n",
    "    \n",
    "    # Product title features\n",
    "    features['title_len'] = features['product_title'].str.len()\n",
    "    features['title_word_count'] = features['product_title'].str.split().str.len()\n",
    "    features['title_unique_words'] = features['product_title'].apply(lambda x: len(set(str(x).lower().split())))\n",
    "    \n",
    "    # Product description features\n",
    "    features['description_len'] = features['product_description'].str.len()\n",
    "    features['description_word_count'] = features['product_description'].str.split().str.len()\n",
    "    \n",
    "    # Brand and color features\n",
    "    features['has_brand'] = (features['product_brand'].str.len() > 0).astype(int)\n",
    "    features['has_color'] = (features['product_color'].str.len() > 0).astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_similarity_features(df):\n",
    "    \"\"\"Create query-product similarity features\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    # Exact matches\n",
    "    features['query_in_title'] = features.apply(\n",
    "        lambda x: 1 if str(x['query']).lower() in str(x['product_title']).lower() else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    features['title_in_query'] = features.apply(\n",
    "        lambda x: 1 if str(x['product_title']).lower() in str(x['query']).lower() else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    # Word overlap features\n",
    "    def word_overlap_ratio(text1, text2):\n",
    "        words1 = set(str(text1).lower().split())\n",
    "        words2 = set(str(text2).lower().split())\n",
    "        if len(words1) == 0 or len(words2) == 0:\n",
    "            return 0\n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        return intersection / union if union > 0 else 0\n",
    "    \n",
    "    def word_jaccard_similarity(text1, text2):\n",
    "        words1 = set(str(text1).lower().split())\n",
    "        words2 = set(str(text2).lower().split())\n",
    "        if len(words1) == 0 and len(words2) == 0:\n",
    "            return 1\n",
    "        if len(words1) == 0 or len(words2) == 0:\n",
    "            return 0\n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        return intersection / union\n",
    "    \n",
    "    print(\"ğŸ“Š Similarity features hesaplanÄ±yor...\")\n",
    "    features['query_title_word_overlap'] = features.apply(\n",
    "        lambda x: word_overlap_ratio(x['query'], x['product_title']), axis=1\n",
    "    )\n",
    "    \n",
    "    features['query_title_jaccard'] = features.apply(\n",
    "        lambda x: word_jaccard_similarity(x['query'], x['product_title']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Brand matching\n",
    "    features['brand_in_query'] = features.apply(\n",
    "        lambda x: 1 if str(x['product_brand']).lower() in str(x['query']).lower() and len(str(x['product_brand'])) > 0 else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create features\n",
    "print(\"âš™ï¸ Basic text features oluÅŸturuluyor...\")\n",
    "df_master_features = create_basic_text_features(df_master)\n",
    "\n",
    "print(\"âš™ï¸ Similarity features oluÅŸturuluyor...\")\n",
    "df_master_features = create_similarity_features(df_master_features)\n",
    "\n",
    "# Add ESCI label encoding\n",
    "esci_mapping = Config.ESCI_MAPPING\n",
    "df_master_features['esci_score'] = df_master_features['esci_label'].map(esci_mapping)\n",
    "\n",
    "print(f\"\\nâœ… Feature engineering tamamlandÄ±!\")\n",
    "print(f\"Final dataset shape: {df_master_features.shape}\")\n",
    "\n",
    "# Show feature summary\n",
    "feature_cols = [col for col in df_master_features.columns if col.endswith(('_len', '_count', '_words', '_overlap', '_jaccard', '_in_', 'has_'))]\n",
    "print(f\"Created {len(feature_cols)} features:\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    if i % 3 == 0:\n",
    "        print()\n",
    "    print(f\"  {col:25}\", end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d0d5b",
   "metadata": {},
   "source": [
    "## 7. Create Task-Specific Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"TASK-SPECIFIC DATASETS\")\n",
    "\n",
    "# Filter for English locale only\n",
    "print(\"ğŸŒ Filtering for English locale (US)...\")\n",
    "df_english = df_master_features[df_master_features['product_locale'] == Config.LANGUAGE].copy()\n",
    "print(f\"English dataset shape: {df_english.shape}\")\n",
    "\n",
    "# Task 1: Query-Product Ranking (small version)\n",
    "print(\"\\nğŸ¯ Task 1: Query-Product Ranking\")\n",
    "print(\"-\" * 40)\n",
    "df_task1 = df_english[df_english['small_version'] == 1].copy()\n",
    "\n",
    "df_task1_train = df_task1[df_task1['split'] == 'train'].copy()\n",
    "df_task1_test = df_task1[df_task1['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"Task 1 Total: {len(df_task1):,} examples\")\n",
    "print(f\"  Train: {len(df_task1_train):,} examples\")\n",
    "print(f\"  Test:  {len(df_task1_test):,} examples\")\n",
    "print(f\"  Unique queries: {df_task1['query_id'].nunique():,}\")\n",
    "\n",
    "# ESCI distribution for Task 1\n",
    "task1_esci = df_task1['esci_label'].value_counts().sort_index()\n",
    "print(\"  ESCI distribution:\")\n",
    "for label, count in task1_esci.items():\n",
    "    print(f\"    {label}: {count:,} ({count/len(df_task1)*100:.1f}%)\")\n",
    "\n",
    "# Task 2: Multi-class Product Classification (large version)\n",
    "print(\"\\nğŸ¯ Task 2: Multi-class Product Classification\")\n",
    "print(\"-\" * 50)\n",
    "df_task2 = df_english[df_english['large_version'] == 1].copy()\n",
    "\n",
    "df_task2_train = df_task2[df_task2['split'] == 'train'].copy()\n",
    "df_task2_test = df_task2[df_task2['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"Task 2 Total: {len(df_task2):,} examples\")\n",
    "print(f\"  Train: {len(df_task2_train):,} examples\")\n",
    "print(f\"  Test:  {len(df_task2_test):,} examples\")\n",
    "print(f\"  Unique queries: {df_task2['query_id'].nunique():,}\")\n",
    "\n",
    "# ESCI distribution for Task 2\n",
    "task2_esci = df_task2['esci_label'].value_counts().sort_index()\n",
    "print(\"  ESCI distribution:\")\n",
    "for label, count in task2_esci.items():\n",
    "    print(f\"    {label}: {count:,} ({count/len(df_task2)*100:.1f}%)\")\n",
    "\n",
    "# Task 3: Product Substitute Identification (large version)\n",
    "print(\"\\nğŸ¯ Task 3: Product Substitute Identification\")\n",
    "print(\"-\" * 45)\n",
    "df_task3 = df_english[df_english['large_version'] == 1].copy()\n",
    "\n",
    "# Create substitute label (1 if S, 0 otherwise)\n",
    "df_task3['substitute_label'] = (df_task3['esci_label'] == 'S').astype(int)\n",
    "\n",
    "df_task3_train = df_task3[df_task3['split'] == 'train'].copy()\n",
    "df_task3_test = df_task3[df_task3['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"Task 3 Total: {len(df_task3):,} examples\")\n",
    "print(f\"  Train: {len(df_task3_train):,} examples\")\n",
    "print(f\"  Test:  {len(df_task3_test):,} examples\")\n",
    "print(f\"  Unique queries: {df_task3['query_id'].nunique():,}\")\n",
    "\n",
    "# Substitute distribution for Task 3\n",
    "substitute_dist = df_task3['substitute_label'].value_counts()\n",
    "print(\"  Substitute distribution:\")\n",
    "print(f\"    Non-Substitute (0): {substitute_dist[0]:,} ({substitute_dist[0]/len(df_task3)*100:.1f}%)\")\n",
    "print(f\"    Substitute (1):     {substitute_dist[1]:,} ({substitute_dist[1]/len(df_task3)*100:.1f}%)\")\n",
    "\n",
    "# Create task datasets dictionary for easy access\n",
    "task_datasets = {\n",
    "    'task1': {\n",
    "        'train': df_task1_train,\n",
    "        'test': df_task1_test,\n",
    "        'full': df_task1,\n",
    "        'target_col': 'esci_score',\n",
    "        'description': 'Query-Product Ranking (Small Version)'\n",
    "    },\n",
    "    'task2': {\n",
    "        'train': df_task2_train,\n",
    "        'test': df_task2_test,\n",
    "        'full': df_task2,\n",
    "        'target_col': 'esci_label',\n",
    "        'description': 'Multi-class Product Classification (Large Version)'\n",
    "    },\n",
    "    'task3': {\n",
    "        'train': df_task3_train,\n",
    "        'test': df_task3_test,\n",
    "        'full': df_task3,\n",
    "        'target_col': 'substitute_label',\n",
    "        'description': 'Product Substitute Identification (Large Version)'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… Task-specific datasets hazÄ±rlandÄ±!\")\n",
    "print(f\"   ğŸ“Š Task 1 (Ranking): {len(df_task1):,} examples\")\n",
    "print(f\"   ğŸ¯ Task 2 (Classification): {len(df_task2):,} examples\")\n",
    "print(f\"   ğŸ” Task 3 (Substitute): {len(df_task3):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDD Cup Official Task Preparation (exactly as specified)\n",
    "print_section(\"KDD CUP OFFICIAL TASK PREPARATION\")\n",
    "\n",
    "# Filter and prepare for Task 1 - exactly as specified\n",
    "print(\"ğŸ¯ Filter and prepare for Task 1\")\n",
    "df_task_1 = df_examples_products[df_examples_products[\"small_version\"] == 1]\n",
    "df_task_1_train = df_task_1[df_task_1[\"split\"] == \"train\"]\n",
    "df_task_1_test = df_task_1[df_task_1[\"split\"] == \"test\"]\n",
    "\n",
    "print(f\"Task 1 (official):\")\n",
    "print(f\"  Total: {len(df_task_1):,}\")\n",
    "print(f\"  Train: {len(df_task_1_train):,}\")\n",
    "print(f\"  Test:  {len(df_task_1_test):,}\")\n",
    "\n",
    "# Filter and prepare data for Task 2 - exactly as specified\n",
    "print(\"\\nğŸ¯ Filter and prepare data for Task 2\")\n",
    "df_task_2 = df_examples_products[df_examples_products[\"large_version\"] == 1]\n",
    "df_task_2_train = df_task_2[df_task_2[\"split\"] == \"train\"]\n",
    "df_task_2_test = df_task_2[df_task_2[\"split\"] == \"test\"]\n",
    "\n",
    "print(f\"Task 2 (official):\")\n",
    "print(f\"  Total: {len(df_task_2):,}\")\n",
    "print(f\"  Train: {len(df_task_2_train):,}\")\n",
    "print(f\"  Test:  {len(df_task_2_test):,}\")\n",
    "\n",
    "# Filter and prepare data for Task 3 - exactly as specified\n",
    "print(\"\\nğŸ¯ Filter and prepare data for Task 3\")\n",
    "df_task_3 = df_examples_products[df_examples_products[\"large_version\"] == 1]\n",
    "df_task_3[\"substitute_label\"] = df_task_3[\"esci_label\"].apply(lambda esci_label: 1 if esci_label == \"S\" else 0)\n",
    "# Note: keeping esci_label column (not deleting as in original spec)\n",
    "df_task_3_train = df_task_3[df_task_3[\"split\"] == \"train\"]\n",
    "df_task_3_test = df_task_3[df_task_3[\"split\"] == \"test\"]\n",
    "\n",
    "print(f\"Task 3 (official):\")\n",
    "print(f\"  Total: {len(df_task_3):,}\")\n",
    "print(f\"  Train: {len(df_task_3_train):,}\")\n",
    "print(f\"  Test:  {len(df_task_3_test):,}\")\n",
    "\n",
    "# Optional: Merge queries with sources (as specified)\n",
    "print(\"\\nğŸ”— Merge queries with sources (optional)\")\n",
    "df_examples_products_source = pd.merge( \n",
    "    df_examples_products, \n",
    "    df_sources, \n",
    "    how='left', \n",
    "    left_on=['query_id'],\n",
    "    right_on=['query_id']\n",
    ")\n",
    "\n",
    "print(f\"With sources shape: {df_examples_products_source.shape}\")\n",
    "\n",
    "print(f\"\\nâœ… KDD Cup official task preparation tamamlandÄ±!\")\n",
    "print(f\"   ğŸ“Š df_task_1: {len(df_task_1):,} examples\")\n",
    "print(f\"   ğŸ¯ df_task_2: {len(df_task_2):,} examples\")\n",
    "print(f\"   ğŸ” df_task_3: {len(df_task_3):,} examples\")\n",
    "print(f\"   ğŸ“‹ df_examples_products_source: {len(df_examples_products_source):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd85c00",
   "metadata": {},
   "source": [
    "## 8. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ce053",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"DATA QUALITY CHECKS\")\n",
    "\n",
    "def perform_quality_checks(task_name, task_data):\n",
    "    \"\"\"Perform comprehensive quality checks on task data\"\"\"\n",
    "    print(f\"\\nğŸ” {task_name} Quality Checks\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    train_df = task_data['train']\n",
    "    test_df = task_data['test']\n",
    "    target_col = task_data['target_col']\n",
    "    \n",
    "    # Basic checks\n",
    "    print(f\"âœ… Train shape: {train_df.shape}\")\n",
    "    print(f\"âœ… Test shape: {test_df.shape}\")\n",
    "    \n",
    "    # Missing values check\n",
    "    train_missing = train_df.isnull().sum().sum()\n",
    "    test_missing = test_df.isnull().sum().sum()\n",
    "    print(f\"âœ… Train missing values: {train_missing}\")\n",
    "    print(f\"âœ… Test missing values: {test_missing}\")\n",
    "    \n",
    "    # Target distribution check\n",
    "    if target_col in train_df.columns:\n",
    "        target_dist = train_df[target_col].value_counts().sort_index()\n",
    "        print(f\"âœ… Target distribution:\")\n",
    "        for value, count in target_dist.items():\n",
    "            print(f\"   {value}: {count:,} ({count/len(train_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Feature completeness\n",
    "    feature_cols = [col for col in train_df.columns if col.endswith(('_len', '_count', '_words', '_overlap', '_jaccard', '_in_', 'has_'))]\n",
    "    feature_missing = train_df[feature_cols].isnull().sum().sum()\n",
    "    print(f\"âœ… Feature columns: {len(feature_cols)}\")\n",
    "    print(f\"âœ… Feature missing values: {feature_missing}\")\n",
    "    \n",
    "    # Query overlap check (no query should be in both train and test)\n",
    "    train_queries = set(train_df['query_id'].unique())\n",
    "    test_queries = set(test_df['query_id'].unique())\n",
    "    query_overlap = len(train_queries.intersection(test_queries))\n",
    "    \n",
    "    if query_overlap == 0:\n",
    "        print(f\"âœ… No query overlap between train/test\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Query overlap detected: {query_overlap} queries\")\n",
    "    \n",
    "    print(f\"âœ… Train unique queries: {len(train_queries):,}\")\n",
    "    print(f\"âœ… Test unique queries: {len(test_queries):,}\")\n",
    "    \n",
    "    return {\n",
    "        'train_shape': train_df.shape,\n",
    "        'test_shape': test_df.shape,\n",
    "        'train_missing': train_missing,\n",
    "        'test_missing': test_missing,\n",
    "        'feature_count': len(feature_cols),\n",
    "        'query_overlap': query_overlap\n",
    "    }\n",
    "\n",
    "# Perform quality checks for all tasks\n",
    "quality_results = {}\n",
    "\n",
    "for task_id, task_data in task_datasets.items():\n",
    "    task_name = f\"Task {task_id[-1]}: {task_data['description']}\"\n",
    "    quality_results[task_id] = perform_quality_checks(task_name, task_data)\n",
    "\n",
    "# Summary of all checks\n",
    "print(f\"\\n\\nğŸ“‹ QUALITY CHECK SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_checks_passed = True\n",
    "for task_id, results in quality_results.items():\n",
    "    task_num = task_id[-1]\n",
    "    status = \"âœ… PASSED\" if results['query_overlap'] == 0 and results['train_missing'] == 0 else \"âš ï¸  WARNING\"\n",
    "    \n",
    "    if results['query_overlap'] > 0 or results['train_missing'] > 0:\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(f\"Task {task_num}: {status}\")\n",
    "    print(f\"  Train: {results['train_shape'][0]:,} rows, {results['train_shape'][1]} cols\")\n",
    "    print(f\"  Test:  {results['test_shape'][0]:,} rows, {results['test_shape'][1]} cols\")\n",
    "    print(f\"  Features: {results['feature_count']}\")\n",
    "    print(f\"  Missing: {results['train_missing']} (train), {results['test_missing']} (test)\")\n",
    "    print(f\"  Query overlap: {results['query_overlap']}\")\n",
    "    print()\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(\"ğŸ‰ All quality checks passed! Data is ready for training.\")\n",
    "else:\n",
    "    print(\"âš ï¸  Some quality issues detected. Please review before training.\")\n",
    "\n",
    "# Create feature list for reference\n",
    "feature_columns = [col for col in df_task1.columns if col.endswith(('_len', '_count', '_words', '_overlap', '_jaccard', '_in_', 'has_'))]\n",
    "print(f\"\\nğŸ“Š Available Features ({len(feature_columns)}):\")\n",
    "for i, col in enumerate(sorted(feature_columns)):\n",
    "    if i % 2 == 0:\n",
    "        print()\n",
    "    print(f\"  {col:35}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0496942",
   "metadata": {},
   "source": [
    "## 9. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"SAVING PROCESSED DATA\")\n",
    "\n",
    "# Create output directories\n",
    "output_dirs = {\n",
    "    'task1': Config.PROCESSED_DATA_DIR / \"task_1\",\n",
    "    'task2': Config.PROCESSED_DATA_DIR / \"task_2\", \n",
    "    'task3': Config.PROCESSED_DATA_DIR / \"task_3\"\n",
    "}\n",
    "\n",
    "for task_dir in output_dirs.values():\n",
    "    task_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "print(\"ğŸ’¾ Saving processed datasets...\")\n",
    "\n",
    "saved_files = []\n",
    "\n",
    "for task_id, task_data in task_datasets.items():\n",
    "    task_dir = output_dirs[task_id]\n",
    "    task_num = task_id[-1]\n",
    "    \n",
    "    # Save train and test sets\n",
    "    train_file = task_dir / f\"train.parquet\"\n",
    "    test_file = task_dir / f\"test.parquet\"\n",
    "    full_file = task_dir / f\"full.parquet\"\n",
    "    \n",
    "    task_data['train'].to_parquet(train_file, index=False)\n",
    "    task_data['test'].to_parquet(test_file, index=False) \n",
    "    task_data['full'].to_parquet(full_file, index=False)\n",
    "    \n",
    "    saved_files.extend([train_file, test_file, full_file])\n",
    "    \n",
    "    print(f\"âœ… Task {task_num} saved:\")\n",
    "    print(f\"   ğŸ“ {train_file.relative_to(Config.BASE_DIR)}\")\n",
    "    print(f\"   ğŸ“ {test_file.relative_to(Config.BASE_DIR)}\")\n",
    "    print(f\"   ğŸ“ {full_file.relative_to(Config.BASE_DIR)}\")\n",
    "\n",
    "# Save feature metadata\n",
    "feature_metadata = {\n",
    "    'feature_columns': feature_columns,\n",
    "    'esci_mapping': Config.ESCI_MAPPING,\n",
    "    'text_fields': ['query', 'product_title', 'product_description', 'product_bullet_point', 'product_brand'],\n",
    "    'created_features': {\n",
    "        'basic_text': [col for col in feature_columns if col.endswith(('_len', '_count', '_words'))],\n",
    "        'similarity': [col for col in feature_columns if col.endswith(('_overlap', '_jaccard', '_in_'))],\n",
    "        'categorical': [col for col in feature_columns if col.startswith('has_')]\n",
    "    },\n",
    "    'preprocessing_info': {\n",
    "        'total_examples': len(df_examples),\n",
    "        'english_examples': len(df_english),\n",
    "        'feature_count': len(feature_columns),\n",
    "        'quality_passed': all_checks_passed\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_file = Config.PROCESSED_DATA_DIR / \"feature_metadata.pkl\"\n",
    "with open(metadata_file, 'wb') as f:\n",
    "    pickle.dump(feature_metadata, f)\n",
    "\n",
    "saved_files.append(metadata_file)\n",
    "print(f\"âœ… Feature metadata saved: {metadata_file.relative_to(Config.BASE_DIR)}\")\n",
    "\n",
    "# Create summary CSV\n",
    "summary_data = []\n",
    "for task_id, task_data in task_datasets.items():\n",
    "    task_num = task_id[-1]\n",
    "    train_df = task_data['train']\n",
    "    test_df = task_data['test']\n",
    "    \n",
    "    summary_data.append({\n",
    "        'task': f\"Task {task_num}\",\n",
    "        'description': task_data['description'],\n",
    "        'train_samples': len(train_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'total_samples': len(task_data['full']),\n",
    "        'unique_queries': task_data['full']['query_id'].nunique(),\n",
    "        'target_column': task_data['target_col'],\n",
    "        'feature_count': len(feature_columns)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_file = Config.PROCESSED_DATA_DIR / \"dataset_summary.csv\"\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "saved_files.append(summary_file)\n",
    "\n",
    "print(f\"âœ… Dataset summary saved: {summary_file.relative_to(Config.BASE_DIR)}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nğŸ“Š DATASET SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "display(summary_df)\n",
    "\n",
    "print(f\"\\nğŸ‰ PREPROCESSING COMPLETED!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"âœ… Processed {len(df_examples):,} original examples\")\n",
    "print(f\"âœ… Created {len(feature_columns)} features\") \n",
    "print(f\"âœ… Generated {len(task_datasets)} task-specific datasets\")\n",
    "print(f\"âœ… Saved {len(saved_files)} files\")\n",
    "print(f\"âœ… Quality checks: {'PASSED' if all_checks_passed else 'WARNINGS'}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Output Directory: {Config.PROCESSED_DATA_DIR}\")\n",
    "print(f\"ğŸ“ Files saved:\")\n",
    "for file_path in saved_files:\n",
    "    file_size = file_path.stat().st_size / (1024*1024)  # MB\n",
    "    print(f\"   {str(file_path.relative_to(Config.BASE_DIR)):50} ({file_size:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nğŸš€ Ready for training! Use the following files:\")\n",
    "print(f\"   Task 1 (Ranking): data/processed/task_1/\")\n",
    "print(f\"   Task 2 (Classification): data/processed/task_2/\")\n",
    "print(f\"   Task 3 (Substitute): data/processed/task_3/\")\n",
    "print(f\"\\nğŸ’¡ Next step: python main.py --task 1\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
